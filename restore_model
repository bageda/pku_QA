所有的数据和代码以mac上为准

输入的语句需要处理才能进行余弦计算  句-》分词— 填充到200—1维list，词转换为索引数— numpy数组

1、载入之前训练的模块
import tensorflow as tf
import numpy as np

#path1 = "/Users/qx/ipython/runs/1470841287/checkpoints/“               #epoch =1500 mac训练出的model
path2 = "/Users/qx/ipython/runs/1470971483/checkpoints/“               #epoch =3000 linux训练出的model

global_step = tf.Variable(0, name="global_step", trainable=False)
W = tf.Variable(-1.0,validate_shape =False,name="embedding/W")
w1 = tf.Variable(-1.0,validate_shape=False,name="conv-maxpool-1/W")
b1 = tf.Variable(-1.0,validate_shape=False,name="conv-maxpool-1/b")
w2 = tf.Variable(-1.0,validate_shape=False,name="conv-maxpool-2/W")
b2 = tf.Variable(-1.0,validate_shape=False,name="conv-maxpool-2/b")
w3 = tf.Variable(-1.0,validate_shape=False,name="conv-maxpool-3/W")
b3 = tf.Variable(-1.0,validate_shape=False,name="conv-maxpool-3/b")
w5 = tf.Variable(-1.0,validate_shape=False,name="conv-maxpool-5/W")
b5 = tf.Variable(-1.0,validate_shape=False,name="conv-maxpool-5/b")

saver = tf.train.Saver()
sess = tf.Session()
attrs = {}

saver.restore(sess,path2 + "model-3000")
with sess.as_default():             #要跟训练session 的环境一致   因为是用with sees.as_default() 训练的
    attrs[W] = sess.run(W)
    attrs[w1] =sess.run(w1)
    attrs[b1] =sess.run(b1)
    attrs[w2] =sess.run(w2)
    attrs[b2] =sess.run(b2)
    attrs[w3] =sess.run(w3)
    attrs[b3] =sess.run(b3)
    attrs[w5] =sess.run(w5)
    attrs[b5] =sess.run(b5)

2、获取cos运算的元素    pooled_flat_1   pooled_len-1

class InsQACNN(object):
    def __init__(
      self, sequence_length, batch_size,
      vocab_size, embedding_size,
      filter_sizes, num_filters):
#参数 200  1  词数  100  [1,2,3,5]  500

        #用户问题,字向量使用embedding_lookup
        self.input_x_1 = tf.placeholder(tf.int32, [batch_size, sequence_length], name="input_x_1")

        # Embedding layer
        with tf.device('/cpu:0'), tf.name_scope("embedding”):        #指定使用设备和名字空间
            W = tf.Variable(
                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),
                name="W")
            chars_1 = tf.nn.embedding_lookup(W, self.input_x_1)
            self.embedded_chars_1 = chars_1

        self.embedded_chars_expanded_1 = tf.expand_dims(self.embedded_chars_1, -1)
        pooled_outputs_1 = []

        for i, filter_size in enumerate(filter_sizes):
            with tf.name_scope("conv-maxpool-%s" % filter_size):
                filter_shape = [filter_size, embedding_size, 1, num_filters]
                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name="W")
                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name="b")
                conv = tf.nn.conv2d(
                    self.embedded_chars_expanded_1,
                    W,
                    strides=[1, 1, 1, 1],
                    padding='VALID',
                    name="conv-1"
                )
                h = tf.nn.relu(tf.nn.bias_add(conv, b), name="relu-1")
                pooled = tf.nn.max_pool(
                    h,
                    ksize=[1, sequence_length - filter_size + 1, 1, 1],
                    strides=[1, 1, 1, 1],
                    padding='VALID',
                    name="poll-1"
                )
                pooled_outputs_1.append(pooled)
        pooled_outputs_1 = []
        num_filters_total = num_filters * len(filter_sizes)
        pooled_reshape_1 = tf.reshape(tf.concat(3, pooled_outputs_1), [-1, num_filters_total])

        #dropout
        pooled_flat_1 = tf.nn.dropout(pooled_reshape_1, self.dropout_keep_prob)

        pooled_len_1 = tf.sqrt(tf.reduce_sum(tf.mul(pooled_flat_1, pooled_flat_1), 1)) #计算向量长度Batch模式


3、得到cos值
member_1 = [pooled_flat_1,pooled_len_1]     #输入问句的
member_2 = [pooled_flat_2,pooled_len_2]     #已收集答案句子的，可提前算好
pooled_mul_12 = tf.reduce_sum(tf.mul(pooled_flat_1, pooled_flat_2), 1) #计算向量的点乘Batch模式
self.cos_12 = tf.div(pooled_mul_12, tf.mul(pooled_len_1, pooled_len_2), name="scores") #计算向量
